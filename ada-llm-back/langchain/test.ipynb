{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sparkai.llm.llm import ChatSparkLLM, ChunkPrintHandler\n",
    "from sparkai.core.messages import ChatMessage\n",
    "from langchain.schema import Document\n",
    "from langchain.schema.retriever import BaseRetriever\n",
    "from typing import List\n",
    "from pydantic import Field\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "import os\n",
    "\n",
    "# 设置 tesseract 的路径\n",
    "os.environ[\"TESSERACT_PATH\"] = r\"C:\\Program Files\\Tesseract-OCR\\tesseract.exe\"\n",
    "os.environ[\"PATH\"] += os.pathsep + r\"C:\\Program Files\\Tesseract-OCR\"\n",
    "\n",
    "SPARKAI_URL = 'wss://spark-api.xf-yun.com/v4.0/chat'\n",
    "SPARKAI_APP_ID = '2087ff0e'\n",
    "SPARKAI_API_SECRET = 'ZDg4Mjg2NDlhNWUwNzA5ZjM5M2YxOTI5'\n",
    "SPARKAI_API_KEY = '562ea31be2df40e0b808ad7d03145cfe'\n",
    "SPARKAI_DOMAIN = '4.0Ultra'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SparkAPIRetriever(BaseRetriever):\n",
    "    document_dir: str = Field(..., description=\"The directory where documents are stored\")\n",
    "    documents: List[Document] = []\n",
    "    loader: DirectoryLoader = None\n",
    "\n",
    "    def __init__(self, document_dir: str, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.document_dir = document_dir\n",
    "        self.loader = DirectoryLoader(path=self.document_dir, show_progress=True)\n",
    "        self.documents = self.load_documents()\n",
    "\n",
    "    def load_documents(self) -> List[Document]:\n",
    "        # 使用 DirectoryLoader 加载文档\n",
    "        return self.loader.load()\n",
    "\n",
    "    def _get_relevant_documents(self, query: str, *, run_manager=None) -> List[Document]:\n",
    "        return [doc for doc in self.documents if query.lower() in doc.page_content.lower()]\n",
    "\n",
    "    async def _aget_relevant_documents(self, query: str, *, run_manager=None) -> List[Document]:\n",
    "        return await super()._aget_relevant_documents(query, run_manager=run_manager)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The MIME type of '..\\\\uploads\\\\二叉树.md' is \"cannot open `..\\\\uploads\\\\\\\\344\\\\272\\\\214\\\\345\\\\217\\\\211\\\\346\\\\240\\\\221.md' (Illegal byte sequence)\". This file type is not currently supported in unstructured.\n",
      "The MIME type of '..\\\\uploads\\\\二叉树2.pdf' is \"cannot open `..\\\\uploads\\\\\\\\344\\\\272\\\\214\\\\345\\\\217\\\\211\\\\346\\\\240\\\\2212.pdf' (Illegal byte sequence)\". This file type is not currently supported in unstructured.\n",
      "100%|██████████| 4/4 [00:00<00:00,  4.28it/s]\n",
      "The MIME type of '..\\\\uploads\\\\二叉树.md' is \"cannot open `..\\\\uploads\\\\\\\\344\\\\272\\\\214\\\\345\\\\217\\\\211\\\\346\\\\240\\\\221.md' (Illegal byte sequence)\". This file type is not currently supported in unstructured.\n",
      "The MIME type of '..\\\\uploads\\\\二叉树2.pdf' is \"cannot open `..\\\\uploads\\\\\\\\344\\\\272\\\\214\\\\345\\\\217\\\\211\\\\346\\\\240\\\\2212.pdf' (Illegal byte sequence)\". This file type is not currently supported in unstructured.\n",
      "100%|██████████| 4/4 [00:00<00:00,  4.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 4 documents\n",
      "Found 3 relevant documents for query '二叉树'\n",
      "Relevant Document: ..\\uploads\\binary_Tree.txt\n",
      "Relevant Document: ..\\uploads\\二叉树.md\n",
      "Relevant Document: ..\\uploads\\二叉树2.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def test_spark_api_retriever():\n",
    "    document_dir = '../uploads'  # 确保这是正确的路径\n",
    "    retriever = SparkAPIRetriever(document_dir=document_dir)\n",
    "\n",
    "    documents = retriever.load_documents()\n",
    "    print(f\"Loaded {len(documents)} documents\")\n",
    "\n",
    "    query = \"二叉树\"\n",
    "    relevant_documents = retriever._get_relevant_documents(query=query)\n",
    "    print(f\"Found {len(relevant_documents)} relevant documents for query '{query}'\")\n",
    "\n",
    "    for doc in relevant_documents:\n",
    "        # print(doc)\n",
    "        print(f\"Relevant Document: {doc.metadata['source']}\")\n",
    "\n",
    "# 运行测试函数\n",
    "test_spark_api_retriever()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SparkAPILLM:\n",
    "    def __init__(self, api_url: str, app_id: str, api_secret: str, api_key: str, domain: str):\n",
    "        self.api_url = api_url\n",
    "        self.app_id = app_id\n",
    "        self.api_secret = api_secret\n",
    "        self.api_key = api_key\n",
    "        self.domain = domain\n",
    "\n",
    "    def generate(self, prompt: str):\n",
    "        spark = ChatSparkLLM(\n",
    "            spark_api_url=self.api_url,\n",
    "            spark_app_id=self.app_id,\n",
    "            spark_api_key=self.api_key,\n",
    "            spark_api_secret=self.api_secret,\n",
    "            spark_llm_domain=self.domain,\n",
    "            streaming=False,\n",
    "        )\n",
    "        messages = [ChatMessage(role=\"user\", content=prompt)]\n",
    "        handler = ChunkPrintHandler()\n",
    "        response = spark.generate([messages], callbacks=[handler])\n",
    "\n",
    "        # 确保正确访问 LLMResult 的内容\n",
    "        generations = response.generations\n",
    "        if generations and generations[0]:\n",
    "            text = generations[0][0].text\n",
    "            # print(f\"Response from LLM: {text}\")\n",
    "            return {\"content\": text}\n",
    "        return {\"content\": \"No response from LLM\"}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RetrievalQA:\n",
    "    def __init__(self, retriever, llm):\n",
    "        self.retriever = retriever\n",
    "        self.llm = llm\n",
    "\n",
    "    def __call__(self, inputs):\n",
    "        query = inputs[\"query\"]\n",
    "        docs = self.retriever._get_relevant_documents(query)\n",
    "        context = \" \".join([doc.page_content for doc in docs])\n",
    "        response = self.llm.generate(context + \"\\n\\n\" + query)\n",
    "        return response[\"content\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_rag_chain():\n",
    "    retriever = SparkAPIRetriever(\n",
    "        document_dir='../uploads'\n",
    "    )\n",
    "    llm = SparkAPILLM(\n",
    "        api_url=SPARKAI_URL,\n",
    "        app_id=SPARKAI_APP_ID,\n",
    "        api_secret=SPARKAI_API_SECRET,\n",
    "        api_key=SPARKAI_API_KEY,\n",
    "        domain=SPARKAI_DOMAIN\n",
    "    )\n",
    "    qa_chain = RetrievalQA(retriever=retriever, llm=llm)\n",
    "    return qa_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The MIME type of '..\\\\uploads\\\\二叉树.md' is \"cannot open `..\\\\uploads\\\\\\\\344\\\\272\\\\214\\\\345\\\\217\\\\211\\\\346\\\\240\\\\221.md' (Illegal byte sequence)\". This file type is not currently supported in unstructured.\n",
      "The MIME type of '..\\\\uploads\\\\二叉树2.pdf' is \"cannot open `..\\\\uploads\\\\\\\\344\\\\272\\\\214\\\\345\\\\217\\\\211\\\\346\\\\240\\\\2212.pdf' (Illegal byte sequence)\". This file type is not currently supported in unstructured.\n",
      "100%|██████████| 4/4 [00:00<00:00,  4.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**二叉树的每个节点最多可以有两个子节点**。在数据结构中，二叉树是一种常见的树形结构，它的每个节点包含一个数据元素以及两个指向其子节点的链接，分别称为“左子节点”和“右子节点”。\n",
      "\n",
      "下面将详细探讨关于二叉树的性质、类型及其它相关概念：\n",
      "\n",
      "1. **基本性质**\n",
      "   - **度的限制**：二叉树的每个节点最多只能有两个子节点。\n",
      "   - **层次节点数**：第i层上的最大节点数目为2^(i-1)^，其中i是从1开始的层数。\n",
      "   - **深度与节点数的关系**：深度为k的二叉树至多有2^k^ - 1个节点。\n",
      "   - **终端节点与分支节点关系**：如果n0表示终端节点（叶子节点）的数量，n2表示度数为2的节点数量，则n0 = n2 + 1。\n",
      "\n",
      "2. **特殊类型**\n",
      "   - **满二叉树**：每一层都被完全填满的二叉树，其节点数目符合2^h^ - 1的规律，其中h为树的高度。\n",
      "   - **完全二叉树**：除了最后一层可能没有被完全填满，其余每一层都被完全填满的二叉树，且最后一层的节点都靠左排列。\n",
      "\n",
      "为了进一步理解二叉树的遍历方式，可以考虑以下几点：\n",
      "\n",
      "- **前序遍历**：先访问根节点，再递归访问左子树，最后递归访问右子树。\n",
      "- **中序遍历**：先递归访问左子树，再访问根节点，最后递归访问右子树。\n",
      "- **后序遍历**：先递归访问左子树，再递归访问右子树，最后访问根节点。\n",
      "\n",
      "总的来说，二叉树作为一种基础而重要的数据结构，在计算机科学领域中有着广泛的应用。它的特性包括每个节点最多有两个子节点，并且存在多种特殊的二叉树结构以适应不同的应用场景。二叉树的遍历方法是理解和操作这种数据结构的关键，其中包括了递归和迭代两种主要的实现方式。\n"
     ]
    }
   ],
   "source": [
    "# 测试 RAG 链\n",
    "rag_chain = create_rag_chain()\n",
    "inputs = {\"query\": \"二叉树每个节点最多有几个孩子\"}\n",
    "response = rag_chain(inputs)\n",
    "print(response)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "adaLLM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
